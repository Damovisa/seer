{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.0.53\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register/Reference a Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "#                                             datastore_name='seerdata', \n",
    "#                                             container_name='your azure blob container name',\n",
    "#                                             account_name='your storage account name', \n",
    "#                                             account_key='your storage account key',\n",
    "#                                             create_if_not_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workspacefilestore': <azureml.data.azure_storage_datastore.AzureFileDatastore at 0x14518696358>,\n",
       " 'workspaceblobstore': <azureml.data.azure_storage_datastore.AzureBlobDatastore at 0x1451869f240>,\n",
       " 'halworkspacestorage__datasets': <azureml.data.azure_storage_datastore.AzureBlobDatastore at 0x145186965c0>,\n",
       " 'seerdata': <azureml.data.azure_storage_datastore.AzureBlobDatastore at 0x14518696cf8>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# workspace\n",
    "ws = Workspace.from_config()\n",
    "ws.datastores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "datastore = ws.datastores['seerdata']\n",
    "datareference = DataReference(\n",
    "    datastore=datastore,\n",
    "    data_reference_name=\"seerdata\")\n",
    "\n",
    "# compute target\n",
    "compute = ws.compute_targets['gandalf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacosburritos_dataset = PipelineData(\n",
    "    \"training_set\",\n",
    "    datastore=datastore,\n",
    "    is_directory=True)\n",
    "\n",
    "fetchStep = PythonScriptStep(\n",
    "    name=\"Data Fetch\",\n",
    "    script_name=\"fetch.py\",\n",
    "    arguments=[\"--target_path\", tacosburritos_dataset, \"--categories\", \"tacos\", \"burrito\"],\n",
    "    inputs=[],\n",
    "    outputs=[tacosburritos_dataset],\n",
    "    compute_target=compute,\n",
    "    source_directory=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Process Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacosburritos_tfrecords = PipelineData(\n",
    "    \"tfrecords_set\",\n",
    "    datastore=datastore,\n",
    "    is_directory=True\n",
    ")\n",
    "\n",
    "prep = Estimator(source_directory='.',\n",
    "                      compute_target=compute,\n",
    "                      entry_script='prep.py',\n",
    "                      use_gpu=True,\n",
    "                      pip_requirements_file='requirements.txt')\n",
    "\n",
    "prepStep = EstimatorStep(\n",
    "    name='Data Preparation',\n",
    "    estimator=prep,\n",
    "    estimator_entry_script_arguments=[\"--source_path\", tacosburritos_dataset, \n",
    "                                      \"--target_path\", tacosburritos_tfrecords],\n",
    "    inputs=[tacosburritos_dataset],\n",
    "    outputs=[tacosburritos_tfrecords],\n",
    "    compute_target=compute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacosburritos_models = PipelineData(\n",
    "    \"models\",\n",
    "    datastore=datastore,\n",
    "    is_directory=True\n",
    ")\n",
    "\n",
    "train = Estimator(source_directory='.',\n",
    "                      compute_target=compute,\n",
    "                      entry_script='train.py',\n",
    "                      use_gpu=True,\n",
    "                      pip_requirements_file='requirements.txt')\n",
    "\n",
    "trainStep = EstimatorStep(\n",
    "    name='Model Training',\n",
    "    estimator=train,\n",
    "    estimator_entry_script_arguments=[\"--source_path\", tacosburritos_tfrecords, \n",
    "                                      \"--target_path\", tacosburritos_models,\n",
    "                                      \"--epochs\", 5,\n",
    "                                      \"--batch\", 10,\n",
    "                                      \"--lr\", 0.001],\n",
    "    inputs=[tacosburritos_tfrecords],\n",
    "    outputs=[tacosburritos_models],\n",
    "    compute_target=compute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=[fetchStep, prepStep, trainStep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Data Fetch [28c1c694][5d4d5572-5d60-4dd6-a4ff-1cb22d8105da], (This step will run and generate new outputs)\n",
      "Created step Data Preparation [4ba3cc0b][1968a44b-d27e-46d3-b0f0-4fe06408301f], (This step will run and generate new outputs)\n",
      "Created step Model Training [4a688288][bac02f9b-de98-49fe-b655-3eda322fe3f6], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: 8762f0b9-4c74-455e-b85b-72178fa54f67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea97f073fe684835bada819e1b03796e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Submit the pipeline to be run\n",
    "pipeline_run1 = Experiment(ws, 'seer').submit(pipeline1)\n",
    "RunDetails(pipeline_run1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
